{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from smart_open import open\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from coatiLDM.common.utils import utc_epoch_now, batch_iterable\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.Crippen import MolLogP\n",
    "from rdkit.Chem import Draw\n",
    "from coatiLDM.models.io import load_score_model_from_model_doc, load_due_cg_from_model_doc\n",
    "import pandas as pd\n",
    "from coatiLDM.constants import FIGURE_DATA_PATH, QED_OPT_DOCS, COATI2_DOCS\n",
    "from coatiLDM.common.s3 import load_figure_file\n",
    "\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.DataStructs.cDataStructs import TanimotoSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "uncond, _, _ = load_score_model_from_model_doc(QED_OPT_DOCS['score_model'])\n",
    "guide = load_due_cg_from_model_doc(QED_OPT_DOCS['guide'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "qed_data_path = './qed_opt/'\n",
    "\n",
    "qed_smiles = load_figure_file('qed_start_smiles.csv',local_dir=qed_data_path,filetype='csv',has_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC(=O)NCCNC(=O)c1cnn(-c2ccc(C)c(Cl)c2)c1C1CC1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C[C@@H](C(=O)C1=c2ccccc2=[NH+]C1)[NH+]1CCC[C@@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCN(C[C@@H]1CCOC1)C(=O)c1ccnc(Cl)c1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cc1ccccc1C[S@](=O)CCCc1ccccc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CSCC(=O)NNC(=O)c1c(O)cc(Cl)cc1Cl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>O=C(CCCn1c(=O)oc2ccccc21)Nc1ccc(Cl)cc1F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>C[C@H](C#N)Sc1nc(-c2ccccc2)cs1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>CS(=O)(=O)CCCOc1ccc(-n2cncn2)cc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>OCc1cccc(-c2ncccn2)c1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>OCc1cn2c(n1)OC(Cl)=CC2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0\n",
       "0        CC(=O)NCCNC(=O)c1cnn(-c2ccc(C)c(Cl)c2)c1C1CC1\n",
       "1    C[C@@H](C(=O)C1=c2ccccc2=[NH+]C1)[NH+]1CCC[C@@...\n",
       "2                  CCN(C[C@@H]1CCOC1)C(=O)c1ccnc(Cl)c1\n",
       "3                        Cc1ccccc1C[S@](=O)CCCc1ccccc1\n",
       "4                     CSCC(=O)NNC(=O)c1c(O)cc(Cl)cc1Cl\n",
       "..                                                 ...\n",
       "795            O=C(CCCn1c(=O)oc2ccccc21)Nc1ccc(Cl)cc1F\n",
       "796                     C[C@H](C#N)Sc1nc(-c2ccccc2)cs1\n",
       "797                   CS(=O)(=O)CCCOc1ccc(-n2cncn2)cc1\n",
       "798                              OCc1cccc(-c2ncccn2)c1\n",
       "799                             OCc1cn2c(n1)OC(Cl)=CC2\n",
       "\n",
       "[800 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qed_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "qed_smiles['clean_smiles'] = qed_smiles.apply(lambda x: Chem.MolToSmiles(Chem.MolFromSmiles(x[0])),axis=1)\n",
    "qed_smiles['no_iso_smiles'] = qed_smiles.apply(lambda x: Chem.MolToSmiles(Chem.MolFromSmiles(x[0]),isomericSmiles=False),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_list = qed_smiles['clean_smiles'].tolist()\n",
    "iso_smiles = qed_smiles['no_iso_smiles'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_recs = [{'SMILES': x, 'no_iso': y} for x,y in zip(smiles_list,iso_smiles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from s3://terray-ml/ben/paper_resources/models/qed_doc.pt\n",
      "Loading tokenizer coati2_12_12 from s3://terray-ml/ben/paper_resources/models/qed_doc.pt\n",
      "number of parameters: 50.44M\n",
      "number of parameters Total: xformer: 54.81M \n",
      "vocab_name not found in tokenizer_vocabs, trying to load from file\n",
      "Freezing encoder\n",
      "56385536 params frozen!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from coatiLDM.models.coati.io import load_coati2\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "MODEL_DOC = COATI2_DOCS['qed_doc']\n",
    "encoder, tokenizer = load_coati2(MODEL_DOC,\n",
    "                                    freeze = True,\n",
    "                                    device = DEVICE)\n",
    "#encoder = torch.compile(encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.77s/it]\n"
     ]
    }
   ],
   "source": [
    "# This is lazy and assumes everything tokenizes (It does)\n",
    "for chunk in tqdm(batch_iterable(smiles_recs, 2048)):\n",
    "    toks, _ = tokenizer.batch_smiles([row['SMILES'] for row in chunk])\n",
    "    res = encoder.encode_tokens(toks.to(DEVICE), tokenizer).cpu().numpy()\n",
    "    for i, entry in enumerate(chunk):\n",
    "        entry['emb_smiles'] = res[i,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coatiLDM.models.diffusion_models.ddpm_sample_routines import ddpm_cg_nearby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncond = uncond.to(DEVICE)\n",
    "guide = guide.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.DataStructs.cDataStructs import TanimotoSimilarity\n",
    "\n",
    "def check_criterion(targ_smi, gen_smiles):\n",
    "    targ_mol = Chem.MolFromSmiles(targ_smi)\n",
    "    gen_mols = [Chem.MolFromSmiles(x) for x in gen_smiles]\n",
    "    gen_mols = [x for x in gen_mols if x is not None]\n",
    "\n",
    "    # Calculate 1024-bit Morgan fingerprint for target molecule\n",
    "    targ_fp = AllChem.GetMorganFingerprintAsBitVect(targ_mol, radius=2, nBits=2048)\n",
    "\n",
    "    for mol in gen_mols:\n",
    "        qed_score = QED.qed(mol)\n",
    "        if qed_score >= 0.9:\n",
    "            # Calculate 1024-bit Morgan fingerprint for generated molecule\n",
    "            #print('cool')\n",
    "            gen_fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=2048)\n",
    "            # Calculate Tanimoto similarity\n",
    "            similarity = TanimotoSimilarity(targ_fp, gen_fp)\n",
    "            if similarity >= 0.4:\n",
    "                return Chem.MolToSmiles(mol), qed_score\n",
    "\n",
    "    return None, None  # If no molecule meets the criteria\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOUND 1 out of 800\n",
      "MISSED 0 out of 800\n",
      "untested: 799\n",
      "oracle calls 400 for CC(=O)NCCNC(=O)c1cnn(-c2ccc(C)c(Cl)c2)c1C1CC1\n",
      "total found 1 out of 800\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m weight \u001b[38;5;129;01min\u001b[39;00m weights:\n\u001b[1;32m     21\u001b[0m     attempt \u001b[38;5;241m=\u001b[39m ddpm_cg_nearby(uncond_score_net\u001b[38;5;241m=\u001b[39muncond,emb_batch\u001b[38;5;241m=\u001b[39mrep_emb,T_start\u001b[38;5;241m=\u001b[39mT,cg_due\u001b[38;5;241m=\u001b[39mguide,targets\u001b[38;5;241m=\u001b[39mcond_targ,cg_weight\u001b[38;5;241m=\u001b[39mweight)\n\u001b[0;32m---> 22\u001b[0m     pred_smiles \u001b[38;5;241m=\u001b[39m \u001b[43mforce_decode_valid_batch_efficient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattempt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_attempts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     smi_match, smi_qed \u001b[38;5;241m=\u001b[39m check_criterion(smi[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSMILES\u001b[39m\u001b[38;5;124m'\u001b[39m],pred_smiles)\n\u001b[1;32m     24\u001b[0m     oracle_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n",
      "File \u001b[0;32m~/coati_diffusion/coatiLDM/data/decoding.py:27\u001b[0m, in \u001b[0;36mforce_decode_valid_batch_efficient\u001b[0;34m(V, encoder, tokenizer, max_attempts, inv_temp, k, noise_scale, chiral, silent)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m vectors\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chiral:\n\u001b[0;32m---> 27\u001b[0m     regen_smiles \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhcoati_to_2d_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43minv_temp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minv_temp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnoise_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_scale\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     regen_smiles \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mhclip_to_2d_batch(\n\u001b[1;32m     36\u001b[0m     vectors, \n\u001b[1;32m     37\u001b[0m     tokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     noise_scale\u001b[38;5;241m=\u001b[39mnoise_scale\n\u001b[1;32m     41\u001b[0m     )\n",
      "File \u001b[0;32m~/coati_diffusion/coatiLDM/models/coati/transformer_only.py:187\u001b[0m, in \u001b[0;36mCOATI_Smiles_Inference.hcoati_to_2d_batch\u001b[0;34m(self, h_coati, tokenizer, fill_in_from, noise_scale, inv_temp, k, do_suffix, keep_special, return_tokens)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m h_token\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m h_token\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxformer\u001b[38;5;241m.\u001b[39mn_embd\n\u001b[0;32m--> 187\u001b[0m generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_top_k_with_inj_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_prebatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43minv_temp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minv_temp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43minj_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43minj_payload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m smiles_list \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    197\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdecode(token_out, special\u001b[38;5;241m=\u001b[39mkeep_special)\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token_out \u001b[38;5;129;01min\u001b[39;00m generation\n\u001b[1;32m    199\u001b[0m ]\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_tokens:\n",
      "File \u001b[0;32m~/coati_diffusion/coatiLDM/models/coati/smiles_xformer.py:396\u001b[0m, in \u001b[0;36mRotarySmilesTransformer.generate_top_k_with_inj_batch\u001b[0;34m(self, prefix, stop_token, pad_token, inv_temp, k, inj_token, inj_payload, as_tensor)\u001b[0m\n\u001b[1;32m    394\u001b[0m     x \u001b[38;5;241m=\u001b[39m prefix_x\n\u001b[1;32m    395\u001b[0m \u001b[38;5;66;03m# batch x len(seq) x num_tokens\u001b[39;00m\n\u001b[0;32m--> 396\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxformer_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;66;03m# logits->batch_size x k , inds-> batch_size x k\u001b[39;00m\n\u001b[1;32m    398\u001b[0m logits_topk, inds_topk \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(\n\u001b[1;32m    399\u001b[0m     logits[:, \u001b[38;5;28mlen\u001b[39m(prefix) \u001b[38;5;241m+\u001b[39m idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m], k\u001b[38;5;241m=\u001b[39mk, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    400\u001b[0m )\n",
      "File \u001b[0;32m~/coati_diffusion/coatiLDM/models/coati/smiles_xformer.py:297\u001b[0m, in \u001b[0;36mRotarySmilesTransformer.xformer_blocks\u001b[0;34m(self, x, apply_norm, output_logits)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mxformer_blocks\u001b[39m(\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, apply_norm: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, output_logits: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    295\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh:\n\u001b[0;32m--> 297\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m apply_norm:\n\u001b[1;32m    299\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_f(x)\n",
      "File \u001b[0;32m~/diff_pub/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/diff_pub/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/coati_diffusion/coatiLDM/models/coati/basic_transformer.py:171\u001b[0m, in \u001b[0;36mRotaryBlock.forward\u001b[0;34m(self, x, rotary_embedding)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, rotary_embedding: RotaryEmbedding):\n\u001b[0;32m--> 171\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotary_embedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlpf(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x))\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/diff_pub/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/diff_pub/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from coatiLDM.data.decoding import force_decode_valid_batch_efficient\n",
    "smiles_results = {}\n",
    "#smiles_results = pickle.load(open('smiles_results_best.pkl','rb'))\n",
    "weights = [10,30,50]\n",
    "batch_size = 100\n",
    "cdf_targ = .995\n",
    "MAX_ORACLE_CALLS = 50000\n",
    "#cond = embed_scalar(torch.tensor([cdf_targ]*num_per_run),embedding_dim=32)\n",
    "cond_targ = torch.tensor([cdf_targ]*batch_size).reshape(-1,1).to(DEVICE)\n",
    "for idx, smi in enumerate(smiles_recs):\n",
    "    if smi['SMILES'] in smiles_results and smiles_results[smi['SMILES']] is not None:\n",
    "        print('skipping')\n",
    "        continue\n",
    "    smiles_results[smi['SMILES']] = None\n",
    "    rep_emb = torch.tensor(np.tile(smi['emb_smiles'],(batch_size,1))).to(DEVICE)\n",
    "    found = False\n",
    "    oracle_calls =0\n",
    "    while oracle_calls < MAX_ORACLE_CALLS and not found:\n",
    "        for T in [50,100,150,200]:\n",
    "            for weight in weights:\n",
    "                attempt = ddpm_cg_nearby(uncond_score_net=uncond,emb_batch=rep_emb,T_start=T,cg_due=guide,targets=cond_targ,cg_weight=weight)\n",
    "                pred_smiles = force_decode_valid_batch_efficient(attempt,encoder,tokenizer,max_attempts=10, silent=True)\n",
    "                smi_match, smi_qed = check_criterion(smi['SMILES'],pred_smiles)\n",
    "                oracle_calls += batch_size\n",
    "                if smi_match is not None:\n",
    "                    \n",
    "                    smiles_results[smi['SMILES']] = (smi_match,smi_qed,weight)\n",
    "                    print(f'FOUND {len([x for x in smiles_results if smiles_results[x] is not None])} out of 800')\n",
    "                    print(f'MISSED {len([x for x in smiles_results if smiles_results[x] is None])} out of 800')\n",
    "                    print(f'untested: {800-len(smiles_results)}')\n",
    "                    found = True\n",
    "                    break\n",
    "                if oracle_calls >= MAX_ORACLE_CALLS:\n",
    "                    break\n",
    "            if found or oracle_calls >= MAX_ORACLE_CALLS:\n",
    "                break\n",
    "        print(f'oracle calls {oracle_calls} for {smi[\"SMILES\"]}')\n",
    "    print(f'total found {len([x for x in smiles_results if smiles_results[x] is not None])} out of 800')\n",
    "    if not found:\n",
    "        print(f'not found {smi[\"SMILES\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_results = load_figure_file('qed_opt_smiles.pkl',local_dir=qed_data_path,filetype='pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765\n"
     ]
    }
   ],
   "source": [
    "print(len([x for x in paper_results.values() if x is not None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95625\n"
     ]
    }
   ],
   "source": [
    "print(765/len(paper_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage success: 0.95625\n"
     ]
    }
   ],
   "source": [
    "# validate paper results\n",
    "for k in paper_results:\n",
    "    targ = Chem.MolFromSmiles(k)\n",
    "    if paper_results[k] is None:\n",
    "        continue\n",
    "    gen = Chem.MolFromSmiles(paper_results[k][0])\n",
    "    gen_fp = AllChem.GetMorganFingerprintAsBitVect(gen, radius=2, nBits=2048)\n",
    "    targ_fp = AllChem.GetMorganFingerprintAsBitVect(targ, radius=2, nBits=2048)\n",
    "    # Calculate Tanimoto similarity\n",
    "    similarity = TanimotoSimilarity(targ_fp, gen_fp)\n",
    "    if similarity < 0.4:\n",
    "        raise ValueError('missed sim cutoff')\n",
    "    qed_score = QED.qed(gen)\n",
    "    if qed_score < 0.9:\n",
    "        raise ValueError('missed qed cutoff')\n",
    "passed_percent = len([x for x in paper_results.values() if x is not None])/len(paper_results)\n",
    "print(f'percentage success: {passed_percent}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_pub",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
